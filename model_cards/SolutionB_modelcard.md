---
{}
---
language: en
license: cc-by-4.0
tags:
- text-classification
repo: https://github.com/kseifo/Natural-Language-Inference

---

# Model Card for g64462ks-t94363ha-NLI

<!-- Provide a quick summary of what the model is/does. -->

Decomposable Attention model with a pre-trained transformer embedding layer.


## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

The decomposable attention model's architecture involves three main steps: Attend, which uses neural attention to softly align subphrases between two sentences; Compare, where these aligned subphrases are separately compared using feed-forward networks; and Aggregate, which combines the comparison vectors to predict the relationship between the sentences
. This architecture allows for parallel processing across sentence length and achieves strong results with fewer parameters and focusing on pairwise alignment and comparison. This model is based on the model described in the 2016 paper by Parikh et al., and adds onto it a pre-trained embedding layer from a DeBERTa-v3 transformer.

- **Developed by:** Kareem Seifo and Hala Saffarini
- **Language(s):** English
- **Model type:** Supervised
- **Model architecture:** Decomposable Attention with Pre-trained Transformer Embeddings

### Model Resources

<!-- Provide links where applicable. -->

- **Paper or documentation:** https://arxiv.org/abs/1606.01933

## Training Details

### Training Data

<!-- This is a short stub of information on the training data that was used, and documentation related to data pre-processing or additional filtering (if applicable). -->

The training data consists of 24,432 pairs of texts provided by the COMP34812 Team, with each pair including a hypothesis, a premise, and a prediction column. The prediction column indicates whether the hypothesis is entailed by the premise.

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Training Hyperparameters

<!-- This is a summary of the values of hyperparameters used in training the model. -->


      - learning_rate: 1e-05
      - train_batch_size: 32
      - eval_batch_size: 32
      - num_epochs: 15
      - dropout_rate: 0.3
      - hidden_size: 512
      - max_sequence_length: 128

#### Speeds, Sizes, Times

<!-- This section provides information about how roughly how long it takes to train the model and the size of the resulting model. -->


      - overall training time: 15 minutes
      - duration per training epoch: 1 minute
      - model size: 28MB

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data & Metrics

#### Testing Data

<!-- This should describe any evaluation data used (e.g., the development/validation set provided). -->

A development set amounting to 6,736 pairs of texts provided by the COMP34812 Team, with each pair including a hypothesis, a premise, and a prediction column.


#### Metrics

<!-- These are the evaluation metrics being used. -->


      - Micro Precision
      - Macro Recall
      - Macro F1 
      - Weighted Precision
      - Weighted Macro Precision
      - Weighted F1
      - Matthews Correlation Coefficient (MCC)
      - Accuracy

### Results

The model obtained an accuracy of 87.71%, a macro precision of 87.69%, a macro recall of 87.71%, a macro F1 of 87.70%, a weighted macro precision of 87.72%, a weighted macro recall of 87.71%, a weighted macro F1 of 87.71%, and a Matthews correlation coefficient of 0.75.

### Additional Evaluation
An additional evaluation was conducted on an augmented version of the development dataset. This augmented dataset was created by applying synonym replacement to the premise and hypothesis fields of each sample with a 25% probability per eligible word. Common conjunctions, short words, and punctuation were excluded from replacement. For each original example, one or two augmented variants were generated by selectively modifying either the premise, the hypothesis, or both. This approach aimed to assess the model's robustness to lexical variation and ensure its performance remains consistent when evaluated on semantically equivalent, yet lexically altered, inputs.

#### Results on Augmented Dataset
The model achieved an overall accuracy of 81.05% on the augmented dataset. Class-wise, it attained a precision of 0.79, recall of 0.84, and F1-score of 0.81 for class 0, while for class 1, the precision was 0.83, recall 0.79, and F1-score 0.81.

## Technical Specifications

### Hardware


      - RAM: at least 16 GB
      - Storage: at least 1GB,
      - GPU: V100

### Hardware Used (Colab Pro Subscription)
      - RAM: 40 GB
      - Storage: 235 GB
      - GPU: A100

### Software


      - scikit-learn (>=0.24.2)
      - pandas (>=1.3.5)
      - numpy (>=1.21.2)
      - tensorflow (latest)
      - keras (latest)
      

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

- **Input Length:**  
  Any inputs (concatenation of two sequences) longer than 128 subwords will be truncated by the model.

- **Handling Class Imbalance:**  
  If you decide to re-train the model on a different dataset, make sure to use the `calculate_normalized_class_weights()` function to compute class weights. This will help address any imbalances in the dataset by ensuring that underrepresented classes receive appropriate emphasis during training.

## Additional Information

<!-- Any other information that would be useful for other people to know. -->
  - The hyperparameters were determined by experimentation with different values.
  - The run times are based on the A100 GPU with 40 GB RAM (Google ColabPro).
