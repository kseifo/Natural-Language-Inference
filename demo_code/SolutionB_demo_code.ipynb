{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Get drive data"
      ],
      "metadata": {
        "id": "UmrmLCvUG9Su"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFgDD61e47Ij",
        "outputId": "232b72e1-5c1b-4caa-ea52-dbf44709d7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "save_path = '/content/drive/MyDrive/nlu'\n",
        "os.makedirs(save_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "SqdKI2O7G9BF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8dWpo19X4pin"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_CLASSES = 2\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "DROPOUT_RATE = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "GPiBVk-UG8ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, Dot, Softmax, Reshape, Permute,\n",
        "    Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Lambda, BatchNormalization, LSTM, Bidirectional\n",
        ")\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from keras.config import enable_unsafe_deserialization\n",
        "from keras.saving import register_keras_serializable\n"
      ],
      "metadata": {
        "id": "zX8TZSOpG7xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of data and creation of model"
      ],
      "metadata": {
        "id": "BInA3j_pG95_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rcOpjtYSoI-5"
      },
      "outputs": [],
      "source": [
        "def prepare_deberta_data(data, tokenizer, max_length=128):\n",
        "\n",
        "    # Encode premises as lists\n",
        "    premises = data['premise'].fillna('').astype(str).tolist()\n",
        "    hypotheses = data['hypothesis'].fillna('').astype(str).tolist()\n",
        "\n",
        "    # Tokenize inputs\n",
        "    encoded = tokenizer(\n",
        "        premises, hypotheses,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def create_decomposable_attention_model(transformer_model, hidden_dim, dropout_rate, max_seq_length, num_classes=2):\n",
        "\n",
        "    # Define inputs\n",
        "    input_ids = Input(shape=(max_seq_length,), dtype='int32', name='input_ids')\n",
        "    attention_mask = Input(shape=(max_seq_length,), dtype='int32', name='attention_mask')\n",
        "    token_type_ids = Input(shape=(max_seq_length,), dtype='int32', name='token_type_ids')\n",
        "\n",
        "    # Convert attention_mask to float32\n",
        "    @keras.saving.register_keras_serializable()\n",
        "    def cast_to_float(x):\n",
        "      return tf.cast(x, tf.float32)\n",
        "\n",
        "    attention_mask_float = Lambda(cast_to_float)(attention_mask)\n",
        "\n",
        "    # Method to handle the transformer model call in sequence_output layer\n",
        "    @keras.saving.register_keras_serializable()\n",
        "    def get_transformer_embeddings(inputs):\n",
        "        return transformer_model(\n",
        "            input_ids=inputs[0],\n",
        "            attention_mask=inputs[1],\n",
        "            token_type_ids=inputs[2]\n",
        "        )[0]\n",
        "\n",
        "    embedding_dim = transformer_model.config.hidden_size\n",
        "\n",
        "    # Get transformer embeddings\n",
        "    sequence_output = Lambda(\n",
        "        get_transformer_embeddings,\n",
        "        output_shape=(max_seq_length, embedding_dim)\n",
        "    )([input_ids, attention_mask, token_type_ids])\n",
        "\n",
        "    # Split premise and hypothesis with token_type_ids\n",
        "    @register_keras_serializable()\n",
        "    def cast_to_float0(x):\n",
        "      return tf.cast(tf.equal(x, 0), tf.float32)\n",
        "\n",
        "    @register_keras_serializable()\n",
        "    def cast_to_float1(x):\n",
        "      return tf.cast(tf.equal(x, 1), tf.float32)\n",
        "\n",
        "    premise_mask = Lambda(cast_to_float0)(token_type_ids)\n",
        "    hypothesis_mask = Lambda(cast_to_float1)(token_type_ids)\n",
        "\n",
        "    @register_keras_serializable()\n",
        "    def multiply_pair(x):\n",
        "        return x[0] * x[1]\n",
        "\n",
        "    # Include the attention mask\n",
        "    premise_mask = Lambda(multiply_pair)([premise_mask, attention_mask_float])\n",
        "    hypothesis_mask = Lambda(multiply_pair)([hypothesis_mask, attention_mask_float])\n",
        "\n",
        "    @register_keras_serializable()\n",
        "    def expand_last_dim(x):\n",
        "        return tf.expand_dims(x, axis=-1)\n",
        "\n",
        "    # Reshape masks\n",
        "    premise_mask_expanded = Lambda(expand_last_dim)(premise_mask)\n",
        "    hypothesis_mask_expanded = Lambda(expand_last_dim)(hypothesis_mask)\n",
        "\n",
        "    # Extract premise and hypothesis embeddings\n",
        "    premise_embedded = Lambda(multiply_pair)([sequence_output, premise_mask_expanded])\n",
        "    hypothesis_embedded = Lambda(multiply_pair)([sequence_output, hypothesis_mask_expanded])\n",
        "\n",
        "    # Encoding layer\n",
        "    premise_encoded = Dense(hidden_dim, activation='tanh')(premise_embedded)\n",
        "    premise_encoded = Dropout(dropout_rate)(premise_encoded)\n",
        "\n",
        "    hypothesis_encoded = Dense(hidden_dim, activation='tanh')(hypothesis_embedded)\n",
        "    hypothesis_encoded = Dropout(dropout_rate)(hypothesis_encoded)\n",
        "\n",
        "    # Attention mechanism\n",
        "    @register_keras_serializable()\n",
        "    def compute_attention(inputs):\n",
        "        p_enc, h_enc = inputs\n",
        "        return tf.matmul(p_enc, tf.transpose(h_enc, perm=[0, 2, 1]))\n",
        "\n",
        "    attention_scores = Lambda(compute_attention)([premise_encoded, hypothesis_encoded])\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    @register_keras_serializable()\n",
        "    def apply_softmax_1(x):\n",
        "        return tf.nn.softmax(x, axis=-1)\n",
        "    @register_keras_serializable()\n",
        "    def apply_softmax_2(x):\n",
        "        return tf.nn.softmax(x, axis=1)\n",
        "\n",
        "    premise_attention = Lambda(apply_softmax_1)(attention_scores)\n",
        "    hypothesis_attention = Lambda(apply_softmax_2)(attention_scores)\n",
        "\n",
        "    # Get attended vectors\n",
        "    @register_keras_serializable()\n",
        "    def get_attended_1(inputs):\n",
        "        att, h_enc = inputs\n",
        "        return tf.matmul(att, h_enc)\n",
        "    @register_keras_serializable()\n",
        "    def get_attended_2(inputs):\n",
        "        att, p_enc = inputs\n",
        "        return tf.matmul(tf.transpose(att, perm=[0, 2, 1]), p_enc)\n",
        "\n",
        "    attended_hypothesis = Lambda(get_attended_1)([premise_attention, hypothesis_encoded])\n",
        "    attended_premise = Lambda(get_attended_2)([hypothesis_attention, premise_encoded])\n",
        "\n",
        "    # Combine original and attended vectors\n",
        "    enhanced_premise = Concatenate()([premise_encoded, attended_hypothesis])\n",
        "    enhanced_hypothesis = Concatenate()([hypothesis_encoded, attended_premise])\n",
        "\n",
        "    # Compare step\n",
        "    compared_premise = Dense(hidden_dim, activation='tanh')(enhanced_premise)\n",
        "    compared_premise = Dropout(dropout_rate)(compared_premise)\n",
        "\n",
        "    compared_hypothesis = Dense(hidden_dim, activation='tanh')(enhanced_hypothesis)\n",
        "    compared_hypothesis = Dropout(dropout_rate)(compared_hypothesis)\n",
        "\n",
        "    # Aggregate step\n",
        "    @register_keras_serializable()\n",
        "    def pooling_with_mask(inputs):\n",
        "        compared, mask = inputs\n",
        "        sum_values = tf.reduce_sum(compared * mask, axis=1)\n",
        "        count = tf.reduce_sum(mask[:,:,0], axis=1, keepdims=True)\n",
        "        return sum_values / (count + 1e-10)\n",
        "\n",
        "    aggregated_premise = Lambda(pooling_with_mask)([compared_premise, premise_mask_expanded])\n",
        "    aggregated_hypothesis = Lambda(pooling_with_mask)([compared_hypothesis, hypothesis_mask_expanded])\n",
        "\n",
        "    # Combine aggregated vectors\n",
        "    merged = Concatenate()([aggregated_premise, aggregated_hypothesis])\n",
        "\n",
        "    # Final classification layers\n",
        "    dense = Dense(hidden_dim, activation='tanh')(merged)\n",
        "    dense = Dropout(dropout_rate)(dense)\n",
        "    output = Dense(num_classes, activation='softmax')(dense)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=output)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get pre-trained tokenizer and model"
      ],
      "metadata": {
        "id": "3w0kUyQyG-yB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTKQauOU41iz",
        "outputId": "bf528298-e775-4065-93a1-0b72d158d0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFDebertaV2Model.\n",
            "\n",
            "All the layers of TFDebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v3-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run model on test data"
      ],
      "metadata": {
        "id": "kZy0A7E2G_HH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_FM7qvlnT-q",
        "outputId": "4ee4f9d3-049d-44ad-e8ee-a42f33fed2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 216ms/step\n"
          ]
        }
      ],
      "source": [
        "# Create model with hyperparameters\n",
        "model = create_decomposable_attention_model(\n",
        "    transformer_model=transformer_model,\n",
        "    hidden_dim=HIDDEN_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n",
        "\n",
        "# Get saved model weights\n",
        "enable_unsafe_deserialization()\n",
        "model.load_weights(\"/content/drive/MyDrive/nlu/attmodel/finalemodelB.weights.h5\")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlu/test.csv')\n",
        "\n",
        "# Tokenize and encode\n",
        "test_inputs = prepare_deberta_data(df, tokenizer)\n",
        "\n",
        "# Predict class probabilities\n",
        "y_probs = model.predict([\n",
        "    test_inputs['input_ids'],\n",
        "    test_inputs['attention_mask'],\n",
        "    test_inputs['token_type_ids']\n",
        "])\n",
        "\n",
        "# Convert to predicted class\n",
        "y_labels = np.argmax(y_probs, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put results in a csv file"
      ],
      "metadata": {
        "id": "FO0ot7XgG_8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to a DataFrame\n",
        "predictions_df = pd.DataFrame({'predictions': y_labels})\n",
        "\n",
        "# Save as csv\n",
        "predictions_df.to_csv('/content/drive/MyDrive/nlu/predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "IqgzIacuv_QO"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}